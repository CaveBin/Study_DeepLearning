{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成式深度学习\r\n",
    "---\r\n",
    "* 文本生成\r\n",
    "* `DeepDream`\r\n",
    "* 神经风格迁移\r\n",
    "* 变分自编码器\r\n",
    "* 生成式对抗网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.1 生成式深度学习用于序列生成的简史"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.2 如何生成序列数据\r\n",
    "---\r\n",
    "#### As usual when working with text data, tokens are typically words or characters, and any network that can model the probability of the next token given the previous ones is called a `language model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.3 采样策略的重要性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [C] 12.1 对于不同`softmax temperature`,对概率分布进行重新加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_distribution(original_distribution, temperature=0.5):\r\n",
    "    # original_distribution是由概率值组成的一堆NumPy数组,这些概率值之和必须等于1\r\n",
    "    # temperature是一个因子,用于定量描述输出分布的熵\r\n",
    "    distribution = np.log(original_distribution) / temperature\r\n",
    "    distribution = np.exp(distribution)\r\n",
    "\r\n",
    "    # 返回原始分布重新加权后的结果.\r\n",
    "    return distribution / np.sum(distribution)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.4 用`keras`实现文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 准备数据 -- 使用 `IMDB`影评数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [C] 12.3 利用文本文件创建数据集(一个文件既一个样本)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30250 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.utils.text_dataset_from_directory(directory='../11Chapter/aclImdb', label_mode=None, batch_size=256)\r\n",
    "dataset = dataset.map(lambda x:tf.strings.regex_replace(x, '<br />', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [C] 12.4 准备`TextVectorization`层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length    = 100\r\n",
    "vocab_size         = 15000  # 只考虑前 15000个最常见单词，其他单词被视为未登陆词元[UNK]\r\n",
    "text_vectorization = TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\r\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [C] 12.5 创建语言模型数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lm_dataset(text_batch):\r\n",
    "    \r\n",
    "    vectorized_sequences = text_vectorization(text_batch)  # 将文本批量转换为整数序列批量\r\n",
    "    x                    = vectorized_sequences[:, :-1]    # 通过删掉序列中最后一个单词来创建输入\r\n",
    "    y                    = vectorized_sequences[:, 1:]     # 通过将序列偏移1个单词来创建目标\r\n",
    "\r\n",
    "    return x, y\r\n",
    "\r\n",
    "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 基于`Transformer`的序列到序列模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\r\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        \r\n",
    "        self.token_embeddings    = layers.Embedding(input_dim=input_dim      , output_dim=output_dim)  # 用于保存词元索引\r\n",
    "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)  # 用于保存词元位置\r\n",
    "        self.sequence_length     = sequence_length\r\n",
    "        self.input_dim           = input_dim\r\n",
    "        self.output_dim          = output_dim\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        length             = tf.shape(inputs)[-1]\r\n",
    "        positions          = tf.range(start=0, limit=length, delta=1)\r\n",
    "        embedded_tokens    = self.token_embeddings(inputs)\r\n",
    "        embedded_positions = self.position_embeddings(positions)\r\n",
    "        \r\n",
    "        return embedded_tokens + embedded_positions\r\n",
    "\r\n",
    "    def compute_mask(self, inputs, mask=None):\r\n",
    "        \r\n",
    "        return tf.math.not_equal(inputs, 0)\r\n",
    "\r\n",
    "    # 实现序列化，以便保存模型\r\n",
    "    def get_config(self):\r\n",
    "        config = super().get_config()\r\n",
    "        \r\n",
    "        config.update({\r\n",
    "            \"output_dim\"     : self.output_dim,\r\n",
    "            \"sequence_length\": self.sequence_length,\r\n",
    "            \"input_dim\"      : self.input_dim,\r\n",
    "        })\r\n",
    "\r\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\r\n",
    "    \r\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.embed_dim   = embed_dim\r\n",
    "        self.dense_dim   = dense_dim\r\n",
    "        self.num_heads   = num_heads\r\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n",
    "        self.dense_proj  = keras.Sequential([layers.Dense(dense_dim, activation='relu'),  layers.Dense(embed_dim), ])\r\n",
    "        self.layernorm_1 = layers.LayerNormalization()\r\n",
    "        self.layernorm_2 = layers.LayerNormalization()\r\n",
    "        self.layernorm_3 = layers.LayerNormalization()\r\n",
    "        self.supports_masking = True\r\n",
    "    \r\n",
    "    def get_config(self):\r\n",
    "        \r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"embed_dim\": self.embed_dim,\r\n",
    "            \"num_heads\": self.num_heads,\r\n",
    "            \"dense_dim\": self.dense_dim,\r\n",
    "        })\r\n",
    "\r\n",
    "        return config\r\n",
    "    \r\n",
    "    def get_causal_attention_mask(self, inputs):\r\n",
    "\r\n",
    "        input_shape = tf.shape(inputs)\r\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\r\n",
    "        i    = tf.range(sequence_length)[:, tf.newaxis]\r\n",
    "        j    = tf.range(sequence_length)\r\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\r\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\r\n",
    "        mult = tf.concat([tf.expand_dims(batch_size, -1),\r\n",
    "                        tf.constant([1, 1], dtype=tf.int32)], axis=0)\r\n",
    "        \r\n",
    "        return tf.tile(mask, mult)\r\n",
    "    \r\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\r\n",
    "        \r\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\r\n",
    "        \r\n",
    "        if mask is not None:\r\n",
    "            padding_mask = tf.cast(\r\n",
    "                    mask[:, tf.newaxis, :], dtype=\"int32\")\r\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\r\n",
    "        \r\n",
    "        attention_output_1 = self.attention_1(\r\n",
    "            query=inputs,\r\n",
    "            value=inputs,\r\n",
    "            key=inputs,\r\n",
    "            attention_mask=causal_mask)\r\n",
    "        \r\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\r\n",
    "        \r\n",
    "        attention_output_2 = self.attention_2(\r\n",
    "            query=attention_output_1,\r\n",
    "            value=encoder_outputs,\r\n",
    "            key=encoder_outputs,\r\n",
    "            attention_mask=padding_mask,\r\n",
    "        )\r\n",
    "        \r\n",
    "        attention_output_2 = self.layernorm_2(\r\n",
    "            attention_output_1 + attention_output_2)\r\n",
    "        proj_output = self.dense_proj(attention_output_2)\r\n",
    "        \r\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [C] 12.6 基于`Transformer`的简单语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim  = 256\r\n",
    "latent_dim = 2048\r\n",
    "num_heads  = 2\r\n",
    "\r\n",
    "inputs  = keras.Input(shape=(None,), dtype='int64')\r\n",
    "x       = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\r\n",
    "x       = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\r\n",
    "outputs = layers.Dense(vocab_size, activation='softmax')(x)  # 对词表中的所有单词做 softmax 运算，对每个输出序列时间步都进行计算\r\n",
    "model   = keras.Model(inputs, outputs)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_2 (Positi  (None, None, 256)   3865600     ['input_3[0][0]']                \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " transformer_decoder_2 (Transfo  (None, None, 256)   2104576     ['positional_embedding_2[0][0]', \n",
      " rmerDecoder)                                                     'positional_embedding_2[0][0]'] \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, None, 15000)  3855000     ['transformer_decoder_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,825,176\n",
      "Trainable params: 9,825,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.5 带有可变温度采样的文本生成调函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [C] 12.7 文本生成回调函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))  # 一个字典，将单词索引映射为字符串，可用于文本解码\r\n",
    "\r\n",
    "# 从概率分布进行采样，温度可变\r\n",
    "def sample_next(predictions, temperature=1.0):\r\n",
    "\r\n",
    "    predictions = np.asarray(predictions).astype('float64')\r\n",
    "    predictions = np.log(predictions) / temperature\r\n",
    "    exp_preds   = np.exp(predictions)\r\n",
    "    predictions = exp_preds / np.sum(exp_preds)\r\n",
    "    probas      = np.random.multinomial(1, predictions, 1)\r\n",
    "\r\n",
    "    return np.argmax(probas)\r\n",
    "\r\n",
    "class TextGenerator(keras.callbacks.Callback):\r\n",
    "\r\n",
    "    def __init__(self, prompt, generate_length, model_input_length, temperatures=(1.,), print_freq=1):\r\n",
    "        # prompt:提示词，作为文本生成的种子\r\n",
    "        # generate_length:要生成多少个单词\r\n",
    "        # temperatures:用于采样的温度值\r\n",
    "\r\n",
    "        self.prompt             = prompt\r\n",
    "        self.generate_length    = generate_length\r\n",
    "        self.model_input_lenght = model_input_length\r\n",
    "        self.temperatures       = temperatures\r\n",
    "        self.print_freq         = print_freq\r\n",
    "\r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        \r\n",
    "        if (epoch + 1) % self.print_freq != 0:\r\n",
    "            return\r\n",
    "        \r\n",
    "        for temperature in self.temperatures:\r\n",
    "            print('== Generating with temperature', temperature)\r\n",
    "            sentence = self.prompt  # 生成文本时，初始文本为提示词\r\n",
    "            for i in range(self.generate_length):\r\n",
    "                tokenized_sentence = text_vectorization([sentence])\r\n",
    "                predictions        = self.model(tokenized_sentence)\r\n",
    "                next_token         = sample_next(predictions[0, i, :])\r\n",
    "                sampled_token      = tokens_index[next_token]\r\n",
    "                sentence          += ' ' + sampled_token\r\n",
    "\r\n",
    "            print(sentence)\r\n",
    "\r\n",
    "prompt = \"This movie\"\r\n",
    "\r\n",
    "text_gen_callback = TextGenerator(\r\n",
    "    prompt,\r\n",
    "    generate_length=50,\r\n",
    "    model_input_length=sequence_length,\r\n",
    "    temperatures=(0.2, 0.5, 0.7, 1., 1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [C] 12.8 拟合语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 6.4121 == Generating with temperature 0.2\n",
      "This movie movie  different to climb strong film back and africa how you like it a [UNK] copy to whos investigation humorous good one of political uneasy on amazing believe receive were meditation as the james terror once by some girl invited cd it gives lots on tales she cant be\n",
      "== Generating with temperature 0.5\n",
      "This movie was its a a johnny serious is [UNK] one full of along many point bad has that a this sound movie her looks canvas like this some film patricia with wins the the life end of capture highlander all in anything my and sequences everything and is [UNK] for samuels\n",
      "== Generating with temperature 0.7\n",
      "This movie isnt war first and stalks big [UNK] battle of too ray brooks the 3 years the i characters be were conroy in action httpwwwimdbcomtitlett0099422usercomments wouldnt stereotypical i slowly are she considered takes pink truly big quite cold one who steps creasy out alley believing others early role story by richard\n",
      "== Generating with temperature 1.0\n",
      "This movie copy is shop little in killers this zone movie i of have cooks explained fontaine young preserve cigarette about light it all is of california pair one is western httpwwwimdbcomtitlett0069097usercomments art [UNK] horror [UNK] thing [UNK] on is stiff the films pet we lots care that for is to so\n",
      "== Generating with temperature 1.5\n",
      "This movie violin appears lions sheer about muslim quotes movie in were this now promote horror does movies what but was singing these while case if off you to have [UNK] real from united a rest seth average a but [UNK] im out a of fan course of but them observes into\n",
      "119/119 [==============================] - 1217s 10s/step - loss: 6.4121\n",
      "Epoch 2/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 5.7595== Generating with temperature 0.2\n",
      "This movie is is that an play american 70s skirt in nonetheless life fodder i [UNK] havent it ever off made and me perhaps about for a the mess photography penelope was his pretty characters good are story ritchie with of a any movie way well but genre after that punishment it\n",
      "== Generating with temperature 0.5\n",
      "This movie 90s is is the damp dirty should by read many it nazis would and take am rehash nice of more dead facts when he you would like really films well why as and [UNK] france restricted that they has should an not stop ever them in feel anticipation low project\n",
      "== Generating with temperature 0.7\n",
      "This movie is makes not it is for just war appears getting to stereotypical a day nice works sort of of 10 the how show tedious is and a f leopard if the paints world i of found you the warn fact his that man it contemporaries naturally you from see murder\n",
      "== Generating with temperature 1.0\n",
      "This movie is this you jeff ed me slave declared crash this store is pretty usually bad good compelling fan drama narrow and film is shows not a anyone [UNK] are detail leave about that boast made i into do person not have just is 5th hunted manuel review the of really\n",
      "== Generating with temperature 1.5\n",
      "This movie is was not so like many the physically characters men until became i quite have suspenseful been so an similar nobody to worked agree even dawn interesting of it those would elliott sure minutes that forward have after been the filled us what my you favorite when free these months\n",
      "119/119 [==============================] - 1181s 10s/step - loss: 5.7595\n",
      "Epoch 3/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 5.5908== Generating with temperature 0.2\n",
      "This movie is has a a strengths way has again to as see a it [UNK] was [UNK] excessively to her hollywood mind if is you [UNK] can now act i and could so not httpwwwimdbcomtitlett0144142usercomments to the be top but it they [UNK] save for a the film cult based movie\n",
      "== Generating with temperature 0.5\n",
      "This movie is that a said film a just funny a for head the plays west a film low revolting budget well team as it well has but no with chemistry the  innocent [UNK] brothers and [UNK] to each wants in her enough to [UNK] whos they could have been living\n",
      "== Generating with temperature 0.7\n",
      "This movie is has in a my writer filmmakers and go she around thunderbirds the with main lonesome character to war the with lucy a by [UNK] the stephen three [UNK] comedies type with marry her party the suspect story include has a every small ward [UNK] 4 the pic movie as\n",
      "== Generating with temperature 1.0\n",
      "This movie flick i lost was watch a it bit was of like the but once i httpwwwimdbcomtitlett0086856usercomments have during to the live [UNK] well the deanna plot no was if just i an really awful didnt [UNK] know [UNK] that of knowing the with early somewhere gets seeing the this camera\n",
      "== Generating with temperature 1.5\n",
      "This movie short instantly is how a a new cheerleader 6 [UNK] supporting worse also as the good dictator and of energy these on set it of and those directed strictly by in the the [UNK] [UNK] touch the the on scenery me time under you the could cinematography get it a\n",
      "119/119 [==============================] - 1116s 9s/step - loss: 5.5908\n",
      "Epoch 4/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 5.4858== Generating with temperature 0.2\n",
      "This movie must made be me seriously it than as the a asked fair to to be mention cool other list  i the thought installment the of cast the the human events between of the her end after at four gets years trust ago by and the the flow greatest and\n",
      "== Generating with temperature 0.5\n",
      "This movie film is festival excellent so only when for i notices really my really family loved drove jeffrey together washed in neighbor and hook life of than my that seat they to are give not a one way is [UNK] the goes premise in that the surprisingly [UNK] to and the\n",
      "== Generating with temperature 0.7\n",
      "This movie movie is to bad see enough people as i gratuitous cant people get thinking to at kill a but [UNK] i and must httpwwwimdbcomtitlett0338467usercomments say and that kind i of felt [UNK] without [UNK] a with crappy its creating deep some experience ordinary furthermore career that some miyazaki good but\n",
      "== Generating with temperature 1.0\n",
      "This movie is is named one an of even minister in with that how but gone get in dead for of its his most tabloid parts hes actor looking who for looks his to two want deaths to ive strike i for theater the hands keeper in of all [UNK] the character\n",
      "== Generating with temperature 1.5\n",
      "This movie made just me been early an film adaptation to hair watch also for time half the however war the [UNK] [UNK] dog as the an [UNK] elton singular has towards a the winner [UNK] of british the 40s lead [UNK] role and of one a of struck a by very\n",
      "119/119 [==============================] - 1092s 9s/step - loss: 5.4858\n",
      "Epoch 5/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 5.4045== Generating with temperature 0.2\n",
      "This movie film is i taken would a have lot bore of from art financial climax and off not after quite seeing [UNK] the with night this the film previous on movie same perhaps future be httpwwwimdbcomtitlett0272557usercomments filling or to not be a the more club twilight  prior on imdb deaths\n",
      "== Generating with temperature 0.5\n",
      "This movie was is so not much all kind the of first 12 of unlike those the turtles great music movie and if happening you the should first have rate my setup httpwwwimdbcomtitlett0439662usercomments to case know you if watching you movies know and a hate bit these wanting memorable to characters shelter\n",
      "== Generating with temperature 0.7\n",
      "This movie is me likely off to in cartoonish my sometimes imdb before for the and way the through highland my something friends rather read worry the about [UNK] 4 historical attention feature i although cannot i be still in appear the in kid my that own like so the that film\n",
      "== Generating with temperature 1.0\n",
      "This movie movie was ive funny seen and better do to me take they the had beginning the every first [UNK] off if enough you to away watch even the more only barbra easy down to this warn movie them if they you are can some call may or not set to\n",
      "== Generating with temperature 1.5\n",
      "This movie movie is there a are commands wonderful [UNK] charlie opens hicks with entertaining great things idea how provoking do friends see like it the the problem place is there a got lot over leading a corruption orphan if away youve on never throughout family the behind movie the gets luxury\n",
      "119/119 [==============================] - 1099s 9s/step - loss: 5.4045\n",
      "Epoch 6/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 5.3395== Generating with temperature 0.2\n",
      "This movie most didnt macho think freeway betray with is me audiences for as instance the i way knew through in his this first one of of them a it piece managed of to the watch series the but dark than and i superb dont and expected their the jokes way that\n",
      "== Generating with temperature 0.5\n",
      "This movie movie is a the certain first [UNK] two fights [UNK] in program an but old then boy did 17 i minutes cant soon recall as the a performances movie of perhaps a the bit greatest of marching the half brain of it this then movie dont subtly know made that\n",
      "== Generating with temperature 0.7\n",
      "This movie movie was is in fantastic my the first story half a until slightly my to favourite do diving details i of found the it tv had this me to at show the is dire that ease it over haunted inman me week to it become makes my you god want\n",
      "== Generating with temperature 1.0\n",
      "This movie movie is starts ok for so me faithful the to first think part peer my its first suffered [UNK] in just this gets absolutely so simple boring oh [UNK] i my didnt review even i the never more dominant so i it have took gone the after worst someone movie\n",
      "== Generating with temperature 1.5\n",
      "This movie movie had i my was once 100 explores i my recorded opinion when when that watching was it 10 i of would [UNK] ever on happening the but movie i in was the in series film other following words of i that attorney i i hoped tried it now is\n",
      "119/119 [==============================] - 1105s 9s/step - loss: 5.3395\n",
      "Epoch 7/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 5.2835== Generating with temperature 0.2\n",
      "This movie movie is great a movie kind oscar of unique nudity plot way but noticeable lacking in but this any one way like it working is to definitely be trash great and its am misleading not but one i then mean the many actors friday the the voice movie acting i\n",
      "== Generating with temperature 0.5\n",
      "This movie movie was i a was comment on on the the story first line scene was has devoid amy of interrogation the of term 10 at [UNK] night or picturesque at and the her memories friends of we the missed first on episode all of [UNK] this film film ray it\n",
      "== Generating with temperature 0.7\n",
      "This movie movie is is terrible a that little is silly a to truly see important empty substantial nothing ladies else year than for or thirties six homicide hours twist of it a has little been evidently born usually because theodore i job will at be that a in classic oscar disregard\n",
      "== Generating with temperature 1.0\n",
      "This movie action plays will some never emotional used producers the and amateur only at come the back overweight at taken the with same pop guy masters adultery of issues political supposed [UNK] to the be [UNK] able for to tv the writing [UNK] the that times whiny the river [UNK] [UNK]\n",
      "== Generating with temperature 1.5\n",
      "This movie is has a accuracy hilarious great the comedy most i artist tell who by really harlan you miller try who of knows [UNK] about usa 20 it holes has this got film this will however not it only is seems everything to going consider on being a to forest go\n",
      "119/119 [==============================] - 1123s 9s/step - loss: 5.2835\n",
      "Epoch 8/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 5.2337== Generating with temperature 0.2\n",
      "This movie movie is was better the than worse the here director enters and the what first the two film about fans this it the was valid made of you the left worst in part 20 2 minutes soon with i the would french be arms 25 ie minutes it everything were\n",
      "== Generating with temperature 0.5\n",
      "This movie is was terrible typical the of only this praised movie ive but ever all made the i simple doubt is the totally negative heartwarming are and just you before think the that charm it the when worst possible acting no was respect one [UNK] for fought things a the short\n",
      "== Generating with temperature 0.7\n",
      "This movie movie was to amazed sit all through that the just first so saw understood this speaking movie terribly was how apparently a its fairly original at the all only wonderful a and good thrilling from to the either least that dinosaurs was could a see great some soap real characters\n",
      "== Generating with temperature 1.0\n",
      "This movie is was a the slightly worst mediocre movies and ive predictable ever ive the seen film especially the anime whole aimed was of it you first really years fast it forward was would filmed probably throughout watch to this her film hilarious ive an seen old la spoiled [UNK] bad\n",
      "== Generating with temperature 1.5\n",
      "This movie film has started set out on for cable tv and the where worst the flick end made throughout of these these characters boring one is of a some descent filmed to on the this effects movie havent greatest read so about i every was viewing directed of by talent [UNK]\n",
      "119/119 [==============================] - 1146s 10s/step - loss: 5.2337\n",
      "Epoch 9/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 5.1894== Generating with temperature 0.2\n",
      "This movie is that the cabin worst with [UNK] the out bad in every 1991 great is standard when the i movies went ive to ever do seen i the dont theater know when first one half played of by them the but best my film dad has is seen going this\n",
      "== Generating with temperature 0.5\n",
      "This movie dvd was is give awesome my i short had on held board by was leigh about he and made it in off western in with english a [UNK] genre was i httpwwwimdbcomtitlett0845463usercomments caught cool it all and i i exhibit am the now most the mst music and irene it\n",
      "== Generating with temperature 0.7\n",
      "This movie would is have an ever award seen for at example first of of the all blair the movie horse one these of note chance it to as surprise the they range were of the that opening it sequence is as certainly well makes ben sense movies to maybe help because\n",
      "== Generating with temperature 1.0\n",
      "This movie piece seemed of for the when [UNK] its appearances own to i the saw most it funny was until released the it same was compelling a but great having musical serious that not you cut will dictator still of i works figured great it show to and remember thinking period\n",
      "== Generating with temperature 1.5\n",
      "This movie is is easily a the film best that in a movies dark im and not i a just movie enjoy is seeing so films long because story still together a and part delightful 1 than great a songs dvd which the hackman superb and especially are for still its in\n",
      "119/119 [==============================] - 1153s 10s/step - loss: 5.1894\n",
      "Epoch 10/10\n",
      "119/119 [==============================] - ETA: 0s - loss: 5.1496== Generating with temperature 0.2\n",
      "This movie creature is is cliché seen in but its pop the does experimental include genre some with other watch castle 3 and women you will get find what a said whole dont team see start again forever and lighthearted nightbreed and is smiling the dont cars trust are by no one\n",
      "== Generating with temperature 0.5\n",
      "This movie movie was was the ill ending probably with the this effects film made is out amazing that considering carries how a far comedy more but mix i between can the imagine plot somewhere the or tv which take were for surprisingly a noted big that level he that allows was\n",
      "== Generating with temperature 0.7\n",
      "This movie movie vardon is publication way ive up seen its it time as if if in you it are marijuana scenes although of i twist didnt isnt like even a though name you i will guess read you a should [UNK] be the the ending original disappointed commentators because hadnt it\n",
      "== Generating with temperature 1.0\n",
      "This movie is hasnt one [UNK] its [UNK] not film everyone as i far see more a cells strange from the the sopranos 20th for century them twists forever or before eye anymore out does to not the the woman beginning using to it much listens better to i have went been\n",
      "== Generating with temperature 1.5\n",
      "This movie is sucked one in of the time worst around movies spot i with had so heard by of the i french was mark not [UNK] seen on especially dvd when and they honeymoon were believe in that [UNK] felt young any to clue definitely at the best octopus in neil\n",
      "119/119 [==============================] - 1137s 10s/step - loss: 5.1496\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x292a42c94d0>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop')\r\n",
    "model.fit(lm_dataset, epochs=10, callbacks=[text_gen_callback])\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "name": "python3113jvsc74a57bd0e842fc153c48e5d72fdba74c5fa9ec255a93a35200f85dc4905f2030a563e165"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "e842fc153c48e5d72fdba74c5fa9ec255a93a35200f85dc4905f2030a563e165"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}